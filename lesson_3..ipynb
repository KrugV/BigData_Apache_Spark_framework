{
  "metadata": {
    "name": "lesson_3",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Урок 3. Типы данных в Spark. Коллекции как объекты DataFrame. User-Defined Functions\n* По данным habr_data получить таблицу с названиями топ-3 статей (по rating) для каждого автора\n* По данным habr_data получить топ (по встречаемости) английских слов из заголовков.\u003cbr\u003e \n_Возможное решение:_ \u003cbr\u003e\n1) выделение слов с помощью регулярных выражений, \n2) разделение на массивы слов \n3) explode массивовов \n4) группировка с подсчетом встречаемости"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import IntegerType\n\nhabrData \u003d spark.read.option(\"header\", True)\\\n    .option(\"inferSchema\", True)\\\n    .csv(\"/user/admin/habr_data.csv\")\\\n    .withColumn(\"rating\", col(\"rating\").cast(IntegerType()))\\\n    .cache()\n\nhabrData.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nz.show(habrData)"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import col, row_number\nfrom pyspark.sql.window import Window\n\nwindowSpec  \u003d Window.partitionBy(\"author_name\").orderBy(col(\"rating\").desc())\n\nz.show(\n    habrData\\\n        .withColumn(\"row_number\", row_number().over(windowSpec)) \\\n        .where(col(\"row_number\") \u003c\u003d 3)\\\n        .select(\"title\",\"author_name\",\"rating\")\n)\n    \n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import udf\nimport re\n\ndef extract_english_words(title):\n    clean_title \u003d re.sub(\u0027\\W+\u0027, \u0027 \u0027, title.lower())\n    words \u003d clean_title.split()\n    return words\n\nextract_english_words_udf \u003d udf(extract_english_words)\n\nz.show(\n    habrData\\\n        .withColumn(\"words\", extract_english_words_udf(\"title\"))\n        .select(\"words\")\n)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import split, explode, col\n\nz.show(\n    habrData\\\n        .select(explode(split(col(\"title\"), \"\\W+\")).alias(\"word\"))\\\n        .where((col(\"word\") !\u003d \u0027\u0027) \u0026 (col(\"word\").rlike(\"[^0-9]\")))\n        .groupBy(\"word\")\\\n        .count()\\\n        .orderBy(col(\"count\").desc())\n)"
    }
  ]
}